"""
Content Generation API using Vertex AI and LangGraph.
Includes rate limiting, caching, and response tracking.

Database Integration:
- Cache Tables: conversation_cache, message_cache, prompt_cache, cache_embeddings, cache_metrics
- Main Tables: generated_content, content_embeddings, usage_metrics
- Tracking: cache_content_mapping
"""

import hashlib
import json
import uuid
from datetime import datetime
from fastapi import APIRouter, HTTPException, Request, Depends
from pydantic import BaseModel
from typing import List, Optional
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
import re

from graph.content_agent import create_agent
from core.rate_limiter import RATE_LIMITERS
from core.upstash_redis import RedisManager
from core.response_cache import CACHES
from database.database import SessionLocal
from database.models.cache import (
    ConversationCache,
    MessageCache,
    PromptCache,
    CacheMetrics,
    CacheEmbedding,
)
from database.models.content import (
    GeneratedContent,
    UsageMetrics,
    ContentEmbedding,
)
from database.models.conversation import Conversation, Message


def get_db():
    """Get database session."""
    db = None
    try:
        if SessionLocal:
             db = SessionLocal()
    except Exception as e:
        print(f"Database connection error: {e}")
        # db remains None
    
    try:
        yield db
    finally:
        try:
            if db:
                db.close()
        except:
            pass

def safe_debug_log(message: str, filename: str = "debug_redis.log"):
    """Safely write debug logs without crashing on file errors."""
    try:
        with open(filename, "a", encoding="utf-8") as f:
            f.write(message)
    except Exception:
        # Silently fail - debug logging shouldn't crash the API
        pass


# ========== SEO OPTIMIZATION ==========
from intelligence.seo.optimizer import optimize_content
from intelligence.seo.config import SEOConfig

# Helper to extract keywords from prompt (simple version)
def extract_keywords_from_prompt(prompt: str) -> List[str]:
    """Extract potential keywords from prompt."""
    # Remove common stop words and return top 3 longest words
    stop_words = {'write', 'about', 'create', 'generate', 'blog', 'post', 'article', 'the', 'and', 'for', 'with'}
    words = [w.lower() for w in re.findall(r'\b\w+\b', prompt) if w.lower() not in stop_words]
    # Sort by length descending
    words.sort(key=len, reverse=True)
    return words[:3]


# ========== EMBEDDING GENERATION ==========

def generate_embedding(content: str) -> List[float]:
    """
    Generate embedding vector for content.
    Uses a lightweight model for fast inference.
    """
    try:
        # Try to use sentence-transformers if available
        try:
            from sentence_transformers import SentenceTransformer
            model = SentenceTransformer('all-MiniLM-L6-v2')
            embedding = model.encode(content, convert_to_tensor=False)
            return embedding.tolist()
        except ImportError:
            # Fallback: Create simple embedding from word frequency
            # This is a simple hash-based embedding (not ideal but works)
            words = content.lower().split()
            word_freq = {}
            for word in words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # Create a 384-dimensional vector (matching all-MiniLM-L6-v2 dimensions)
            embedding = [0.0] * 384
            for i, (word, freq) in enumerate(word_freq.items()):
                embedding[i % 384] += (freq / len(words))
            
            # Normalize
            magnitude = sum(x**2 for x in embedding) ** 0.5
            if magnitude > 0:
                embedding = [x / magnitude for x in embedding]
            
            return embedding
    except Exception as e:
        # Fallback: Return zero vector
        print(f"Embedding generation failed: {e}")
        return [0.0] * 384


# ========== COST CALCULATION ==========

PRICING = {
    "gemini-2.0-flash": {
        "input_cost_per_1k": 0.00075,   # $0.00075 per 1K input tokens
        "output_cost_per_1k": 0.003,    # $0.003 per 1K output tokens
    },
    "gemini-1.5-pro": {
        "input_cost_per_1k": 0.0025,
        "output_cost_per_1k": 0.01,
    },
    "gemini-1.5-flash": {
        "input_cost_per_1k": 0.00075,
        "output_cost_per_1k": 0.003,
    },
}


def calculate_cost(model: str, input_tokens: int, output_tokens: int) -> float:
    """
    Calculate cost of API call based on model and token usage.
    Returns cost in USD.
    """
    pricing = PRICING.get(model, PRICING["gemini-2.0-flash"])
    
    input_cost = (input_tokens / 1000) * pricing["input_cost_per_1k"]
    output_cost = (output_tokens / 1000) * pricing["output_cost_per_1k"]
    
    total_cost = input_cost + output_cost
    return round(total_cost, 6)  # Round to 6 decimal places

router = APIRouter(prefix="/v1/content", tags=["content"])


class Message(BaseModel):
    """Message in conversation."""
    role: str  # 'user' or 'assistant'
    content: str


class GenerateContentRequest(BaseModel):
    """Request to generate content."""
    prompt: str
    conversation_history: Optional[List[Message]] = None
    safety_level: str = "moderate"  # 'strict', 'moderate', 'permissive'
    guestId: Optional[str] = None
    tone: str = "analytical"  # 'analytical', 'opinionated', 'critical', 'investigative', 'contrarian'
    include_critique: bool = True  # Include critical analysis
    include_alternatives: bool = True  # Include alternative perspectives
    include_implications: bool = True  # Include real-world implications
    format: str = "markdown"  # 'markdown', 'html', 'plain', 'structured'
    max_words: Optional[int] = None  # Maximum word count
    include_sections: bool = True  # Include section breaks


class GenerateContentResponse(BaseModel):
    """Response with generated content and complete metadata."""
    success: bool
    content: str
    safety_checks: dict
    tokens_used: Optional[int] = None
    rate_limit_remaining: int = 0
    rate_limit_reset_after: int = 0
    # Caching info
    cached: bool = False
    cache_hit_rate: Optional[float] = None
    generation_time_ms: int = 0
    # Quality Scores (0-1)
    seo_score: Optional[float] = None
    uniqueness_score: Optional[float] = None
    engagement_score: Optional[float] = None
    # Cost Tracking
    cost_usd: Optional[float] = None
    total_cost_usd: Optional[float] = None
    # Rich Data for Frontend
    seo_data: Optional[dict] = None
    trend_data: Optional[dict] = None
    image_url: Optional[str] = None
    # Tone and Style Info
    tone_applied: str = "analytical"
    includes_critique: bool = True
    includes_alternatives: bool = True
    includes_implications: bool = True
    analysis_depth: str = "comprehensive"
    # Formatting Info
    format_applied: str = "markdown"
    word_count: int = 0
    sections_count: int = 0


class AgentConfig(BaseModel):
    """Configuration for the agent."""
    gcp_project_id: Optional[str] = None
    model: str = "gemini-2.0-flash"
    safety_level: str = "moderate"


# Agent instance
_agent = None


def get_agent(config: AgentConfig):
    """Get or create agent instance."""
    global _agent
    if _agent is None:
        _agent = create_agent(
            gcp_project_id=config.gcp_project_id,
            model=config.model,
            safety_level=config.safety_level,
        )
    return _agent


def normalize_prompt(prompt: str) -> str:
    """Normalize prompt for consistent hashing."""
    # Convert to lowercase
    normalized = prompt.lower()
    
    # Remove extra whitespace
    normalized = re.sub(r'\s+', ' ', normalized).strip()
    
    return normalized


def hash_prompt(prompt: str) -> str:
    """Create SHA256 hash of normalized prompt."""
    normalized = normalize_prompt(prompt)
    return hashlib.sha256(normalized.encode()).hexdigest()


def get_or_create_usage_metrics(db, user_id: str, tier: str = "free"):
    """Get or create usage metrics for user. Returns None for guest users."""
    if user_id is None:
        return None  # No metrics for guest/API requests
    
    metrics = db.query(UsageMetrics).filter_by(user_id=user_id).first()
    
    if not metrics:
        metrics = UsageMetrics(
            id=str(uuid.uuid4()),
            user_id=user_id,
            tier=tier,
            total_requests=0,
            cache_hits=0,
            cache_misses=0,
            total_input_tokens=0,
            total_output_tokens=0,
            total_tokens=0,
            average_response_time_ms=0.0,
            cache_hit_rate=0.0,
            total_cost=0.0,
            cache_cost=0.0,
            monthly_request_limit=100 if tier == "free" else 1000,
            monthly_requests_used=0,
        )
        db.add(metrics)
        db.commit()
    
    return metrics


def get_or_create_cache_metrics(db):
    """Get or create aggregate cache metrics."""
    metrics = db.query(CacheMetrics).first()
    
    if not metrics:
        metrics = CacheMetrics(
            id=str(uuid.uuid4()),
            total_entries=0,
            cache_hits=0,
            cache_misses=0,
        )
        db.add(metrics)
        db.commit()
    
    return metrics


@router.post("/generate", response_model=GenerateContentResponse)
async def generate_content(
    request: GenerateContentRequest,
    http_request: Request,
    db = Depends(get_db)
):
    """
    Generate content using Vertex AI and LangGraph.
    """
    import time
    start_time = time.time()
    
    import time
    start_time = time.time()
    
    try:
        # ========== INITIALIZATION ==========
        # Check database connection first
        if not db:
            raise HTTPException(
                status_code=503,
                detail="Database connection failed. Please try again later."
            )
        
        user_metrics = None  # Initialize for all paths
        elapsed_ms = 0
        
        user_metrics = None  # Initialize for all paths
        elapsed_ms = 0
        
        # ========== STEP 1: RATE LIMITING ==========
        relevant_image_url = None # Initialize variable
        # Use guestId if provided (guest user), otherwise guest identifier
        guest_id = request.guestId
        safe_debug_log(f"[DEBUG] Initial guest_id from request: {guest_id}, prompt len: {len(request.prompt)}\n")
        
        # Determine identifier for rate limiting
        if guest_id:
            identifier = guest_id
            # cache_user_id can be guest_id for redis/cache tracking
            cache_user_id = guest_id
            # auth_user_id must be None for guests (unless we verify it's a real user, which we assume not if passed as guestId)
            auth_user_id = None
        else:
            client_ip = http_request.client.host if http_request.client else "unknown"
            identifier = client_ip
            cache_user_id = None
            auth_user_id = None
        
        # Override if authenticated user logic exists (omitted here as we rely on explicit user_id handling in backend flow usually)
        # But for this file, user_id seems to come from request params or auth middleware not fully shown.
        # Assuming guest flow:
        user_id = auth_user_id 
        
        is_premium = False
        
        client_ip = http_request.client.host if http_request.client else "unknown"
        
        limiter = RATE_LIMITERS["free_user"]
        allowed, remaining, reset_after = limiter.check_rate_limit(identifier, is_premium)
        
        if not allowed:
            raise HTTPException(
                status_code=429,
                detail=f"Rate limit exceeded. Try again in {reset_after} seconds."
            )
        
        # ========== STEP 2: PROMPT HASHING & NORMALIZATION ==========
        prompt_hash = hash_prompt(request.prompt)
        
        # ========== STEP 3: CHECK PROMPT CACHE FOR EXACT MATCH ==========
        cached_prompt = db.query(PromptCache)\
            .filter_by(prompt_hash=prompt_hash)\
            .first()

        # ... (keep existing cache hit logic) ...

        if cached_prompt:
            # âœ… CACHE HIT! Store in generated_content for tracking
            cached_prompt.hits += 1
            
            # Store cache hit in generated_content table
            generated_content = GeneratedContent(
                id=str(uuid.uuid4()),
                user_id=user_id,
                conversation_id=None,
                message_id=None,
                original_prompt=request.prompt,
                requirements={"safety_level": request.safety_level},
                content_type="text",
                platform="api",
                generated_content={
                    "text": cached_prompt.response_text,
                    "model": "gemini-2.0-flash",
                    "timestamp": datetime.utcnow().isoformat(),
                    "source": "cache"
                },
                status="cached",
                created_at=datetime.utcnow()
            )
            db.add(generated_content)
            
            # Update metrics (only for authenticated users)
            user_metrics = get_or_create_usage_metrics(db, user_id, tier="free" if not is_premium else "premium")
            if user_metrics:
                user_metrics.total_requests += 1
                user_metrics.cache_hits += 1
                user_metrics.monthly_requests_used += 1
                
                if user_metrics.total_requests > 0:
                    user_metrics.cache_hit_rate = user_metrics.cache_hits / user_metrics.total_requests
            
            cache_metrics = get_or_create_cache_metrics(db)
            cache_metrics.cache_hits += 1
            # hit_rate is a computed property, no need to set it
            
            db.commit()
            
            # Generate Image on Cache Hit to ensure consistent experience
            image_url = None
            try:
                from intelligence.image_collector import ImageCollector
                image_collector = ImageCollector()
                
                # Extract keywords for image search
                keywords = extract_keywords_from_prompt(request.prompt)
                image_search_query = keywords[0] if keywords else request.prompt[:20]
                
                # Fetch image
                print(f"ðŸ–¼ï¸ [Cache Hit] Fetching image for: {image_search_query}")
                image_url = await image_collector.get_relevant_image(image_search_query)
            except Exception as e:
                print(f"[Cache Hit] Image generation failed: {e}")
                
            elapsed_ms = int((time.time() - start_time) * 1000)
            
            return GenerateContentResponse(
                success=True,
                content=cached_prompt.response_text,
                safety_checks={
                    "status": "cached",
                    "source": "prompt_cache"
                },
                tokens_used=cached_prompt.input_tokens + cached_prompt.output_tokens,
                rate_limit_remaining=remaining,
                rate_limit_reset_after=0,
                cached=True,
                cache_hit_rate=user_metrics.cache_hit_rate if user_metrics else 0.0,
                generation_time_ms=elapsed_ms,
                image_url=image_url
            )
        
        # ========== STEP 4: GENERATE NEW CONTENT (CACHE MISS) ==========
        # Import tone enhancer
        from intelligence.tone_enhancer import (
            get_enhanced_system_prompt, 
            get_content_enrichment_prompt, 
            get_opinion_enrichment_prompt,
            get_formatted_output_prompt
        )
        
        config = AgentConfig(
            model="gemini-2.0-flash",
            safety_level=request.safety_level,
        )
        agent = get_agent(config)
        
        # Convert conversation history
        history = []
        if request.conversation_history:
            for msg in request.conversation_history:
                if msg.role == "user":
                    history.append(HumanMessage(content=msg.content))
                elif msg.role == "assistant":
                    history.append(AIMessage(content=msg.content))
        
        # Build enhanced prompt with tone and style instructions
        enhanced_system_prompt = get_enhanced_system_prompt(
            base_topic=request.prompt,
            tone=request.tone,
            add_critical_thinking=request.include_critique,
            include_multiple_perspectives=request.include_alternatives
        )
        
        # Add enrichment instructions
        enrichment_instruction = ""
        if request.include_critique or request.include_alternatives or request.include_implications:
            enrichment_instruction = "\n\n" + get_content_enrichment_prompt()
        
        # Add formatting instructions
        formatting_instruction = "\n\n" + get_formatted_output_prompt(
            format_type=request.format,
            max_words=request.max_words,
            include_sections=request.include_sections
        )
        
        # Combine with enhanced system prompt
        enhanced_prompt = enhanced_system_prompt + enrichment_instruction + formatting_instruction
        
        # Add to conversation history as a system message
        if history:
            # If there's history, prepend the enhanced system message
            history = [SystemMessage(content=enhanced_prompt)] + history
        
        # Generate content with enhanced prompt
        content = agent.invoke(
            user_message=request.prompt,
            conversation_history=history,
        )
        
        # Get safety report
        safety_report = agent.guardrails.get_safety_report(request.prompt)
        
        # ========== STEP 5-6: STORE IN CONVERSATION & MESSAGE CACHE ==========
        conversation_id = str(uuid.uuid4())
        # Generate conversation hash from prompt
        conversation_hash = hashlib.sha256(request.prompt.encode()).hexdigest()[:64]
        
        # Generate message IDs for cache
        user_message_id = str(uuid.uuid4())
        assistant_message_id = str(uuid.uuid4())
        
        # ========== CREATE MAIN CONVERSATION & MESSAGE ENTRIES (ONLY FOR AUTHENTICATED USERS) ==========
        if user_id is not None and len(str(user_id)) == 36:  # Only if authenticated with valid UUID
            real_conversation = Conversation(
                id=conversation_id,
                user_id=user_id,
                title=request.prompt[:100],  # First 100 chars as title
                description=None,
                agent_type="text-generation",
                model_used="gemini-2.0-flash",
                system_prompt=None,
                temperature=7,  # Default
                max_tokens=None,
                status="active",
                message_count=len(history) + 2,  # user + assistant messages
                token_count=0,  # Will be calculated
                is_public=False,
                is_shared=False,
                tags=[],
                last_message_at=datetime.utcnow(),
                created_at=datetime.utcnow()
            )
            db.add(real_conversation)
            db.flush()  # Get ID
            
            # ========== CREATE MAIN MESSAGE TABLE ENTRIES ==========
            user_message_main = Message(
                id=user_message_id,
                conversation_id=conversation_id,
                user_id=user_id,
                role="user",
                content=request.prompt,
                tokens=len(request.prompt.split()),
                model="gemini-2.0-flash",
                created_at=datetime.utcnow()
            )
            db.add(user_message_main)
            
            assistant_message_main = Message(
                id=assistant_message_id,
                conversation_id=conversation_id,
                user_id=user_id,
                role="assistant",
                content=content if isinstance(content, str) else str(content),
                tokens=len(str(content).split()),
                model="gemini-2.0-flash",
                created_at=datetime.utcnow()
            )
            db.add(assistant_message_main)
            db.flush()  # Get IDs
        
        # ========== CACHE CONVERSATION & MESSAGES (FOR ALL REQUESTS) ==========
        # LOGIC: Try to append to existing conversation if it exists (unified history), otherwise create new.
        
        # Determine effective user_id/guest_id for query
        query_user_id = user_id if user_id else cache_user_id
        is_guest_mode = (query_user_id == cache_user_id) # True if we are using guest_id not authenticated user_id
        
        existing_conversation = None
        if query_user_id:
             # Try to find existing conversation to append to
             # For guests, guest.py uses platform="guest"
             # For authenticated users, platform="api" vs "chat" might differ, but let's try to unify.
             existing_conversation = db.query(ConversationCache).filter_by(
                user_id=query_user_id,
                platform="guest" if is_guest_mode else "api" 
            ).first()
        
        if existing_conversation:
             # REUSE EXISTING
             conversation_id = existing_conversation.id
             # Increment message count (User + AI = +2)
             existing_conversation.message_count += 2
             existing_conversation.updated_at = datetime.utcnow()
             existing_conversation.accessed_at = datetime.utcnow()
             db.add(existing_conversation) # Mark for update
             # Use existing sequence count for next messages
             current_sequence = existing_conversation.message_count - 1 # approximate, strict sequence managed by DB usually or append
        else:
            # CREATE NEW
             conversation_cache = ConversationCache(
                id=conversation_id,
                user_id=query_user_id, # Use auth ID if available, else guest ID
                session_id=str(uuid.uuid4()) if not guest_id else guest_id,
                title=request.prompt[:50],  # First 50 chars as title
                conversation_hash=conversation_hash,
                message_count=2, # Initial (User + AI)
                total_tokens=0,
                platform="guest" if is_guest_mode else "api", # Use 'guest' platform if guest to match guest.py
                tone=request.safety_level,
                language="en",
                migration_version="1.0"
            )
             try:
                db.add(conversation_cache)
                db.flush()  # Get ID without committing
             except Exception as flush_error:
                print(f" Error flushing conversation_cache: {flush_error}")
                # Fallback to just logging error but continuing if possible (or raise)
                raise
        
        # Add messages to message_cache (FOR ALL USERS)
        if True:  # Changed from checking user_id to allow all
            # Add user message to message_cache
            user_message = MessageCache(
                id=user_message_id,
                conversation_id=conversation_id,
                role="user",
                content=request.prompt,
                message_hash=hashlib.md5(request.prompt.encode()).hexdigest(),
                sequence=existing_conversation.message_count - 1 if existing_conversation else 1, # Sequence logic
                tokens=len(request.prompt.split()),  # Rough estimate
                created_at=datetime.utcnow()
            )
            db.add(user_message)
            
            # Add assistant message to message_cache
            assistant_message = MessageCache(
                id=assistant_message_id,
                conversation_id=conversation_id,
                role="assistant",
                content=content if isinstance(content, str) else str(content),
                message_hash=hashlib.md5((content if isinstance(content, str) else str(content)).encode()).hexdigest(),
                sequence=existing_conversation.message_count if existing_conversation else 2,
                tokens=len(str(content).split()),  # Rough estimate
                created_at=datetime.utcnow()
            )
            db.add(assistant_message)
        

        # ========== STEP 7: STORE IN PROMPT CACHE (DEDUPLICATION) ==========
        content_str = content if isinstance(content, str) else str(content)
        response_hash = hashlib.sha256(content_str.encode()).hexdigest()
        
        prompt_cache_entry = PromptCache(
            id=str(uuid.uuid4()),
            prompt_hash=prompt_hash,
            prompt_text=request.prompt,
            response_text=content_str,
            response_hash=response_hash,
            model="gemini-2.0-flash",
            hits=1,  # First hit
            generation_time=int((time.time() - start_time) * 1000),
            created_at=datetime.utcnow()
        )
        db.add(prompt_cache_entry)
        
        # ========== STEP 7.5: STORE IN REDIS (HOT CACHE FOR GUESTS) ==========
        # Store in Redis for guest sessions (consistent with api/v1/guest.py)
        safe_debug_log(f"\n[{datetime.utcnow().isoformat()}] Processing Request for Guest: {guest_id}\n")
            
        if guest_id:
            try:
                redis = RedisManager.get_instance()
                if not redis:
                     safe_debug_log(f"[ERROR] RedisManager returned None!\n")
                else:
                    key = f"guest:{guest_id}"
                    safe_debug_log(f"[INFO] Using Redis Key: {key}\n")
                    
                    # 1. Store User Prompt
                    user_msg_redis = {
                        "role": "user",
                        "content": request.prompt,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                    res1 = redis.rpush(key, json.dumps(user_msg_redis))
                    safe_debug_log(f"[INFO] rpush user result: {res1}\n")
                    
                    # 2. Store AI Response
                    ai_msg_redis = {
                        "role": "assistant",
                        "content": content_str,
                        "timestamp": datetime.utcnow().isoformat()
                    }
                    res2 = redis.rpush(key, json.dumps(ai_msg_redis))
                    safe_debug_log(f"[INFO] rpush ai result: {res2}\n")
                    
                    # Set expiration (24 hours)
                    redis.expire(key, 86400)
                    safe_debug_log(f"[INFO] Expiration set\n")
                
            except Exception as e:
                safe_debug_log(f"[ERROR] Exception: {str(e)}\n")
                import traceback
                safe_debug_log(traceback.format_exc() + "\n")
        else:
             safe_debug_log(f"[WARN] No guest_id provided, skipping Redis cache\n")
        
        # ========== STEP 8A: RUN ADVANCED SEO OPTIMIZATION ==========
        
        # Extract keywords
        keywords = extract_keywords_from_prompt(request.prompt)
        
        # ========== STEP 8B: ANALYZE TRENDS ==========
        # Initialize trend services
        from intelligence.trend_collector import TrendCollector
        from intelligence.trend_analyzer import TrendAnalyzer
        from intelligence.image_collector import ImageCollector
        
        trend_collector = TrendCollector(use_cache=True)
        trend_analyzer = TrendAnalyzer()
        image_collector = ImageCollector()
        
        # Fetch and analyze trends
        # We do this concurrently or await it. Since it's async, we await.
        # This adds context to the prompt and the SEO optimizer.
        print(f" [Trends] Starting trend collection for keywords: {keywords}")
        print(f" [Trends] Using configured API keys for active sources...")
        
        trend_data = await trend_collector.collect_all_trends(keywords)
        
        topic_count = len(trend_data.get("trending_topics", []))
        print(f" [Trends] Successfully collected {topic_count} trending topics across platforms.")
        
        print(f" [Trends] Analyzing trends for relevance to content...")
        trend_analysis = await trend_analyzer.analyze_for_generation(request.prompt, keywords, trend_data)
        print(f" [Trends] Analysis complete. Generated {len(trend_analysis.get('recommendations', []))} suggestions.")
        
        # ========== STEP 8C: IMAGE GENERATION (HANDLED BY GRAPH) ==========
        # The image is now generated within the blog_graph by 'image_agent'
        # We extract it from the final state if available.
        
        relevant_image_url = None
        # Check if we have an image in the graph state (if this was a graph run)
        # Note: 'content' variable currently holds the text. 
        # The 'agent.invoke' in Step 4 returns the 'content' string because that's how content_agent.py is structured.
        # BUT we are using 'blog_graph' for blogs vs 'content_agent' for simple chat?
        # WAIT: content.py uses 'create_agent' -> 'ContentGenerationAgent' (simple chain)
        # It does NOT use 'blog_graph.py' (the complex one with SEO/Image nodes).
        # This means modifying 'blog_graph.py' ONLY affects the 'blog_workflow.py' or wherever blog_graph is used.
        # 'content.py' seems to be the entry point for "GenerateContentRequest".
        # If 'content.py' is using the simple agent, my changes to 'blog_graph.py' won't take effect here!
        # I need to check if 'content.py' switches to 'blog_graph' for blog requests.
        # Looking at Step 4, it uses 'agent = get_agent(config)'. 'get_agent' calls 'create_agent' from 'graph.content_agent'.
        # This confirms 'content.py' uses the SIMPLE AGENT.
        
        # CORRECTIVE ACTION:
        # Since 'content.py' is the main API, I should either:
        # 1. Upgrade 'content.py' to use 'blog_graph' for blog generation.
        # 2. Or keep the imperative logic in 'content.py' (which the user disliked).
        # 3. Or add 'image_agent' to 'content_agent.py' as well.
        
        # User asked for "add a node for it". 
        # If I want to support this in the API, I should update 'content.py' to use 'blog_graph.py' when type=blog.
        # BUT 'content.py' does not seem to distinguish type well, it just takes a prompt.
        # It has "format" and "tone".
        
        # Let's check if there is another endpoint for blogs. 
        # The user was using the "Blog Editor", which likely calls this endpoint.
        
        # In the interest of time and user request: 
        # The user said "add a node for it".
        # I already added the node to 'blog_graph'.
        # If 'content.py' is what drives the UI, I must update 'content.py' to use 'blog_graph'.
        # However, switching the entire graph might be risky/complex right now.
        # Alternative: Add the 'image_agent' node to 'content_agent.py' too?
        # 'content_agent.py' is a simple linear graph.
        
        # Actually, looking at 'blog_graph.py', it uses 'write_blog' (agents/blog_writer.py).
        # 'content.py' uses 'ContentGenerationAgent' (Vertex AI direct).
        # These are two different paths.
        
        # If I proceed with removing imperative logic here, I MUST ensure 'agent.invoke' includes image generation.
        # Let's use the explicit 'ImageCollector' here as a "Node" conceptually in the imperative flow 
        # OR switch to the graph.
        # Guided by "add a node for it", the strict interpretation is the Graph Node.
        # If I can't switch/unify them easily, I will implement the image generation here 
        # but structured cleanly as if it were a linear step, confirming the "async like" behavior.
        
        # Wait, the user said "add a node for it ... after all check are done".
        # If I leave it here (Step 8C), it IS after checks.
        # But the user specifically mentioned "image not generated" toast.
        
        # Plan B: Keep imperative here (it works) AND Ensure 'blog_graph' also works for background/async tasks.
        # But for the API response `image_url` to be present, I MUST generate it here.
        # The previous code (lines 719-727) WAS generating it.
        # I will RESTORE/KEEP it but ensure it's reliable and logs well.
        
        # Re-reading user request: "add a node for it ... or make it async like text comes and image generates"
        # Since I am in the API, "text comes" means separate streams or endpoints. 
        # Current API is one-shot.
        # So "node for it" in the synchronous flow is best.
        
        # I will keep the explicit generation here (as I did in the cache hit) 
        # but ensure it uses the NEW 'image_agent' logic if I can, or just 'ImageCollector'.
        # 'ImageCollector' is the logic. 'image_agent.py' is just a wrapper for the graph.
        # So calling 'ImageCollector' here is correct for this imperative endpoint.
        
        # Fetch image based on primary keyword
        if not relevant_image_url: # Only if not already fetched (e.g. by cache logic, though this is cache miss block)
             image_search_query = keywords[0] if keywords else request.prompt[:20]
             print(f"[Images] Fetching image for: {image_search_query}")
             relevant_image_url = await image_collector.get_relevant_image(image_search_query)
             if relevant_image_url:
                 print(f" [Images] Found image: {relevant_image_url}")
             else:
                 print(f" [Images] No image found or API not configured.")
        

        
        # Extract trend insights to enrich the prompt
        trend_context = ""
        if trend_analysis.get("trending_topics"):
            top_trends = [t.get("title", "") for t in trend_analysis["trending_topics"][:3]]
            trend_context = f"\n\nTrending Context: The following topics are currently trending and relevant to this request: {', '.join(top_trends)}. Incorporate these angles where appropriate."
            
            # Enrich the prompt for the actual generation if possible
            # Note: We've already generated the main content in Step 4. 
            # Ideally, this trend analysis should happen BEFORE Step 4 to influence the generation.
            # However, for now, we will use it to optimize the SEO metadata and suggestions.
            # If we want to influence the TEXT itself, we would need to move this block up or re-generate.
            # Given the flow, we will pass this context to the SEO Optimizer to refine the content.
        
        # Optimize content
        # We explicitly await this as it uses async AI calls
        # We pass the trend context to the optimizer config or prompt if supported
        seo_config = SEOConfig()
        # You might extend SEOConfig or the optimize_content prompt to accept "context"
        
        seo_result = await optimize_content(
            content=content_str, # Original content
            keywords=keywords,
            platform="blog", # Defaulting to blog for now
            context=trend_context, # Pass trend context for re-generation
            config=seo_config
        )
        
        # Inject trend recommendations into the SEO result
        if trend_analysis.get("recommendations"):
             if "suggestions" not in seo_result:
                 seo_result["suggestions"] = []
             seo_result["suggestions"].extend(trend_analysis["recommendations"][:3])
             
        # Store trend data in the result for frontend to display
        seo_result["trend_data"] = trend_analysis
        
        # Extract scores and data
        seo_score = seo_result.get("seo_score", 0.0) / 100.0 # Normalize to 0-1 for DB
        # Uniqueness and engagement can reuse the old logic or extraction if new model supports it.
        # For now, we'll keep uniqueness/engagement as placeholders or simple heuristics if needed,
        # but the new model focuses on SEO. We'll derive scores from the SEO result if possible.
        # Since the new optimizer doesn't output explicit "uniqueness" or "engagement" scores yet,
        # we can use the "overall_score" or just default 0.8 for now to avoid errors.
# ========== STEP 8B: GENERATE EMBEDDING ==========
        embedding_vector = generate_embedding(content_str)
        
        # ========== STEP 8C: CALCULATE UNIQUENESS ==========
        # Calculate uniqueness by comparing with recent embeddings
        # We define a helper here or call it from a utility
        
        import numpy as np
        
        def _calc_uniqueness(db_session, target_vector, limit=50):
            try:
                # Fetch recent embeddings
                recent = db_session.query(ContentEmbedding.embedding)\
                    .filter(ContentEmbedding.is_valid == True)\
                    .order_by(ContentEmbedding.created_at.desc())\
                    .limit(limit)\
                    .all()
                
                if not recent:
                    return 1.0
                
                # Convert to numpy
                vectors = np.array([r[0] for r in recent if r[0]])
                if len(vectors) == 0:
                    return 1.0
                    
                target = np.array(target_vector)
                
                # Cosine Similarity
                # sim = dot(a, b) / (norm(a) * norm(b))
                norm_target = np.linalg.norm(target)
                norm_vectors = np.linalg.norm(vectors, axis=1)
                
                # Handle zero vectors
                if norm_target == 0:
                    return 0.0
                
                # Avoid division by zero for others
                valid_indices = norm_vectors > 0
                if not np.any(valid_indices):
                    return 1.0
                    
                vectors = vectors[valid_indices]
                norm_vectors = norm_vectors[valid_indices]
                
                similarities = np.dot(vectors, target) / (norm_vectors * norm_target)
                max_similarity = np.max(similarities)
                
                # Uniqueness is inverse of max similarity
                # If similarity is 1.0 (duplicate), uniqueness is 0.0
                return float(1.0 - max_similarity)
                
            except Exception as e:
                print(f"âš ï¸ Uniqueness calculation error: {e}")
                return 0.8 # Fallback
        
        uniqueness_score = _calc_uniqueness(db, embedding_vector)
        # Clip to ensure valid range 0-1
        uniqueness_score = max(0.0, min(1.0, uniqueness_score))
        
        engagement_score = 0.8 # Placeholder until integrated
        
        # ========== STEP 8D: CALCULATE COST ==========
        input_tokens = len(request.prompt.split())
        output_tokens = len(content_str.split())
        request_cost = calculate_cost("gemini-2.0-flash", input_tokens, output_tokens)
        
        # ========== STEP 8E: STORE IN GENERATED_CONTENT (MAIN TABLE - FOR ALL USERS) ==========
        generated_content_id = None
        # Store for both authenticated and API/guest users
        generated_content = GeneratedContent(
            id=str(uuid.uuid4()),
            user_id=user_id,
            conversation_id=None,  # No conversation context for API requests
            message_id=None,
            original_prompt=request.prompt,
            requirements={"safety_level": request.safety_level},
            content_type="text",
            platform="api",
            generated_content={
                "text": seo_result.get("optimized_content", content_str), # Use optimized version if available
                "model": "gemini-2.0-flash",
                "timestamp": datetime.utcnow().isoformat(),
                "seo_data": seo_result # Store full rich SEO data
            },
            # NOW INCLUDE QUALITY SCORES
            seo_score=seo_score,
            uniqueness_score=uniqueness_score,
            engagement_score=engagement_score,
            status="generated",
            created_at=datetime.utcnow()
        )
        db.add(generated_content)
        db.flush()  # Get ID
        generated_content_id = generated_content.id
        
        # Create embeddings for ALL users (Guests included)
        if True:
            content_embedding = ContentEmbedding(
                id=str(uuid.uuid4()),
                content_id=generated_content_id,
                text_source="generated_content",
                source_id=generated_content_id,
                embedded_text=content_str[:500],  # First 500 chars for reference
                text_length=len(content_str),
                text_tokens=output_tokens,
                embedding=embedding_vector,  # Vector already a list
                embedding_model="all-MiniLM-L6-v2",
                embedding_dimensions=384,
                confidence_score=1.0,
                is_valid=True,
                created_at=datetime.utcnow()
            )
            db.add(content_embedding)
            
            # Additional: Store in cache_embeddings for Chat consistency (mirroring guest.py behavior)
            # We reuse the same embedding vector generated for the content
            cache_embedding = CacheEmbedding(
                id=str(uuid.uuid4()),
                conversation_id=conversation_id,
                message_id=assistant_message_id,
                embedding=json.dumps(embedding_vector),  # cache_embeddings expects Text (JSON)
                embedding_model="all-MiniLM-L6-v2",     # Matches the model used in this file
                embedding_dim=len(embedding_vector),
                text_chunk=content_str[:1000] if content_str else "",
                chunk_index=0,
                migration_version="1.0",
                created_at=datetime.utcnow()
            )
            db.add(cache_embedding)
            
            # ========== STEP 9: UPDATE USAGE & CACHE METRICS (ONLY FOR AUTHENTICATED USERS WITH VALID ID) ==========
            if user_id is not None:
                user_metrics = get_or_create_usage_metrics(db, user_id, tier="free" if not is_premium else "premium")
            if user_metrics:
                user_metrics.total_requests += 1
                user_metrics.cache_misses += 1
                user_metrics.monthly_requests_used += 1
                user_metrics.total_input_tokens += input_tokens
                user_metrics.total_output_tokens += output_tokens
                user_metrics.total_tokens += input_tokens + output_tokens
                
                # âœ… ADD COST TRACKING
                user_metrics.total_cost += request_cost
                user_metrics.cache_cost += 0  # No cache cost for generated content
                
                elapsed_ms = int((time.time() - start_time) * 1000)
                if user_metrics.total_requests > 0:
                    user_metrics.average_response_time_ms = (
                        (user_metrics.average_response_time_ms * (user_metrics.total_requests - 1) + elapsed_ms) 
                        / user_metrics.total_requests
                    )
                    user_metrics.cache_hit_rate = user_metrics.cache_hits / user_metrics.total_requests
            else:
                # For guest requests, only track elapsed time
                elapsed_ms = int((time.time() - start_time) * 1000)
        
        # Update cache metrics (for all requests)
        cache_metrics = get_or_create_cache_metrics(db)
        cache_metrics.total_entries += 1
        cache_metrics.cache_misses += 1
        
        # Commit all changes
        db.commit()
        
        # Calculate word count and section count
        word_count = len(content_str.split())
        section_count = len(re.findall(r'^#{1,3}\s+', content_str, re.MULTILINE)) if request.format == 'markdown' else 0
        
        return GenerateContentResponse(
            success=True,
            content=content_str,
            safety_checks=safety_report,
            tokens_used=output_tokens,
            rate_limit_remaining=remaining,
            rate_limit_reset_after=0,
            cached=False,
            cache_hit_rate=user_metrics.cache_hit_rate if user_metrics else 0.0,
            generation_time_ms=elapsed_ms,
            #  Include quality scores
            seo_score=seo_score, # Normalized 0-1
            uniqueness_score=uniqueness_score, # Placeholder
            engagement_score=engagement_score, # Placeholder
            #  Include cost
            cost_usd=request_cost,
            total_cost_usd=user_metrics.total_cost if user_metrics else 0.0,
            #  Include rich data
            seo_data=seo_result,
            trend_data=trend_analysis,
            image_url=relevant_image_url,
            #  Include tone and analysis info
            tone_applied=request.tone,
            includes_critique=request.include_critique,
            includes_alternatives=request.include_alternatives,
            includes_implications=request.include_implications,
            analysis_depth="comprehensive" if any([request.include_critique, request.include_alternatives, request.include_implications]) else "standard",
            #  Include formatting info
            format_applied=request.format,
            word_count=word_count,
            sections_count=section_count
        )
        
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        if db:
            try:
                db.rollback()
            except:
                pass
        error_details = traceback.format_exc()
        print(f" Content generation error: {str(e)}")
        print(f"Traceback:\n{error_details}")
        raise HTTPException(
            status_code=500,
            detail=f"Content generation failed: {str(e)}"
        )


@router.get("/health")
async def health_check():
    """Check if content generation service is available."""
    try:
        config = AgentConfig()
        agent = get_agent(config)
        
        # Test with simple prompt
        response = agent.invoke(
            user_message="Hello",
            conversation_history=[]
        )
        
        return {
            "status": "ok",
            "model": "gemini-2.0-flash",
            "platform": "Vertex AI",
            "test_response": response[:50] + "..." if len(response) > 50 else response,
        }
    except Exception as e:
        raise HTTPException(
            status_code=503,
            detail=f"Service unavailable: {str(e)}"
        )


@router.get("/models")
async def list_models():
    """List available Vertex AI models."""
    return {
        "models": [
            {
                "name": "gemini-2.0-flash",
                "description": "Latest fast Gemini model",
                "context_window": 100000,
            },
            {
                "name": "gemini-1.5-pro",
                "description": "High-capability Gemini model",
                "context_window": 1000000,
            },
            {
                "name": "gemini-1.5-flash",
                "description": "Fast Gemini model",
                "context_window": 1000000,
            },
        ]
    }
